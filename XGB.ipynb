{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuH9UwxmqEme"
      },
      "outputs": [],
      "source": [
        "class XGBoostFromScratch():\n",
        "    '''A Custom Implementation of XGBoost with Decision Trees'''\n",
        "\n",
        "    def __init__(self, hyperparams, seed=None):\n",
        "        # Initialize hyperparameters with default values if not provided\n",
        "        self.hyperparams = defaultdict(lambda: None, hyperparams)\n",
        "\n",
        "        # Set subsampling ratio for the training data\n",
        "        self.sample_fraction = self.hyperparams['subsample'] or 1.0\n",
        "\n",
        "        # Learning rate, controls how much to shrink the predictions at each boosting round\n",
        "        self.eta = self.hyperparams['learning_rate'] or 0.3\n",
        "\n",
        "        # Base prediction score (initial prediction for all examples)\n",
        "        self.initial_score = self.hyperparams['base_score'] or 0.5\n",
        "\n",
        "        # Maximum depth of the decision trees (controls model complexity)\n",
        "        self.depth_limit = self.hyperparams['max_depth'] or 5\n",
        "\n",
        "        # Initialize a random number generator (for reproducibility and randomness in subsampling)\n",
        "        self.random_gen = np.random.default_rng(seed=seed)\n",
        "\n",
        "    def fit(self, X, y, objective_function, num_rounds, verbose=False):\n",
        "        '''\n",
        "        Trains the model by building a sequence of decision trees.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features matrix (pandas DataFrame or numpy array).\n",
        "        - y: Target variable (numpy array or pandas Series).\n",
        "        - objective_function: A loss function object that provides gradient and curvature (second derivative).\n",
        "        - num_rounds: Number of boosting rounds (number of trees to build).\n",
        "        - verbose: If True, prints loss at each round for monitoring.\n",
        "        '''\n",
        "\n",
        "        # Initialize predictions with the base score (initial constant value for all data points)\n",
        "        self.trees = []  # To store all decision trees\n",
        "        current_predictions = self.initial_score * np.ones(y.shape)  # Initial predictions\n",
        "\n",
        "        # Training loop for each boosting round (adding one tree per round)\n",
        "        for round_num in range(num_rounds):\n",
        "            # Compute gradients (first derivative of the loss function w.r.t. predictions)\n",
        "            gradients = objective_function.gradient(y, current_predictions)\n",
        "\n",
        "            # Compute curvatures (second derivative of the loss function w.r.t. predictions)\n",
        "            curvatures = objective_function.curvature(y, current_predictions)\n",
        "\n",
        "            # Randomly subsample the data if subsampling is enabled (subsample < 1.0)\n",
        "            selected_indices = None if self.sample_fraction == 1.0 else \\\n",
        "                self.random_gen.choice(len(y), size=math.floor(self.sample_fraction * len(y)), replace=False)\n",
        "\n",
        "            # Build a single decision tree using the gradients and curvatures\n",
        "            tree = DecisionTree(X, gradients, curvatures, self.hyperparams, self.depth_limit, selected_indices)\n",
        "\n",
        "            # Update the predictions with the output from the new tree (shrinkage by learning rate)\n",
        "            current_predictions += self.eta * tree.predict(X)\n",
        "\n",
        "            # Store the tree for future predictions\n",
        "            self.trees.append(tree)\n",
        "\n",
        "            # Optionally print the loss for this round to monitor progress\n",
        "            if verbose:\n",
        "                print(f'[{round_num}] Training Loss = {objective_function.loss(y, current_predictions)}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Makes predictions for the input data by summing up the contributions from all trees.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input data for which predictions are to be made.\n",
        "\n",
        "        Returns:\n",
        "        - Predicted values for the input data.\n",
        "        '''\n",
        "\n",
        "        # Start with the base prediction and add the contributions from all trees\n",
        "        return self.initial_score + self.eta * np.sum([tree.predict(X) for tree in self.trees], axis=0)\n",
        "\n",
        "\n",
        "class DecisionTree():\n",
        "    '''Represents a single decision tree built for a boosting round.'''\n",
        "\n",
        "    def __init__(self, X, gradients, curvatures, hyperparams, depth_limit, selected_indices=None):\n",
        "        '''\n",
        "        Builds a decision tree based on the gradients and curvatures of the loss function.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features matrix (pandas DataFrame or numpy array).\n",
        "        - gradients: Gradients (first derivative of the loss function w.r.t. predictions).\n",
        "        - curvatures: Curvatures (second derivative of the loss function w.r.t. predictions).\n",
        "        - hyperparams: Dictionary of hyperparameters (like max depth, regularization).\n",
        "        - depth_limit: Maximum depth of the tree (how deep the tree can grow).\n",
        "        - selected_indices: Optional subsample of indices for training this tree.\n",
        "        '''\n",
        "\n",
        "        self.hyperparams = hyperparams\n",
        "        self.depth_limit = depth_limit\n",
        "        assert self.depth_limit >= 0, 'Depth must be nonnegative'\n",
        "\n",
        "        # Hyperparameter settings with fallback values\n",
        "        self.min_child_weight = hyperparams['min_child_weight'] or 1.0\n",
        "        self.regularization = hyperparams['reg_lambda'] or 1.0\n",
        "        self.split_penalty = hyperparams['gamma'] or 0.0\n",
        "        self.feature_sampling_rate = hyperparams['colsample_bynode'] or 1.0\n",
        "\n",
        "        # Convert pandas Series to numpy arrays if necessary\n",
        "        if isinstance(gradients, pd.Series):\n",
        "            gradients = gradients.values\n",
        "        if isinstance(curvatures, pd.Series):\n",
        "            curvatures = curvatures.values\n",
        "\n",
        "        # If no subsampling, use all data points\n",
        "        if selected_indices is None:\n",
        "            selected_indices = np.arange(len(gradients))\n",
        "\n",
        "        # Store data and necessary statistics for building the tree\n",
        "        self.X, self.gradients, self.curvatures, self.selected_indices = X, gradients, curvatures, selected_indices\n",
        "        self.sample_size, self.num_features = len(selected_indices), X.shape[1]\n",
        "\n",
        "        # Prediction value for this node, calculated as a leaf node initially\n",
        "        # The formula minimizes the loss function based on gradients and curvatures\n",
        "        self.prediction_value = -gradients[selected_indices].sum() / (curvatures[selected_indices].sum() + self.regularization)\n",
        "\n",
        "        # This variable tracks the best split found during the tree construction\n",
        "        self.best_gain = 0.0\n",
        "\n",
        "        # If the tree can grow deeper, we evaluate possible splits\n",
        "        if self.depth_limit > 0:\n",
        "            self._evaluate_possible_splits()\n",
        "\n",
        "    def _evaluate_possible_splits(self):\n",
        "        '''Evaluates potential splits at each feature and chooses the best one based on gain.'''\n",
        "\n",
        "        # Iterate through all features to find the best split\n",
        "        for feature_idx in range(self.num_features):\n",
        "            self._attempt_split(feature_idx)\n",
        "\n",
        "        # If no good split was found, this node will remain a leaf\n",
        "        if self.is_leaf_node:\n",
        "            return\n",
        "\n",
        "        # Partition the data based on the best split found\n",
        "        feature_values = self.X.values[self.selected_indices, self.best_split_feature]\n",
        "        left_idx = np.nonzero(feature_values <= self.split_threshold)[0]\n",
        "        right_idx = np.nonzero(feature_values > self.split_threshold)[0]\n",
        "\n",
        "        # Recursively grow the left and right child trees\n",
        "        self.left_child = DecisionTree(self.X, self.gradients, self.curvatures, self.hyperparams, self.depth_limit - 1, self.selected_indices[left_idx])\n",
        "        self.right_child = DecisionTree(self.X, self.gradients, self.curvatures, self.hyperparams, self.depth_limit - 1, self.selected_indices[right_idx])\n",
        "\n",
        "    @property\n",
        "    def is_leaf_node(self):\n",
        "        '''Check if the current node is a leaf node (i.e., no further split was found).'''\n",
        "        return self.best_gain == 0.0\n",
        "\n",
        "    def _attempt_split(self, feature_idx):\n",
        "        '''\n",
        "        Tries to find the best split for a given feature by evaluating the potential gain in loss reduction.\n",
        "\n",
        "        Parameters:\n",
        "        - feature_idx: Index of the feature to evaluate.\n",
        "        '''\n",
        "\n",
        "        # Sort the data by the selected feature to evaluate potential splits\n",
        "        feature_values = self.X.values[self.selected_indices, feature_idx]\n",
        "        gradients, curvatures = self.gradients[self.selected_indices], self.curvatures[self.selected_indices]\n",
        "        sorted_indices = np.argsort(feature_values)\n",
        "        sorted_gradients, sorted_curvatures, sorted_feature_values = gradients[sorted_indices], curvatures[sorted_indices], feature_values[sorted_indices]\n",
        "\n",
        "        # Calculate total gradients and curvatures for the entire node\n",
        "        total_gradient, total_curvature = gradients.sum(), curvatures.sum()\n",
        "        left_gradient, left_curvature = 0.0, 0.0  # Cumulative values for left child\n",
        "        right_gradient, right_curvature = total_gradient, total_curvature  # Cumulative values for right child\n",
        "\n",
        "        # Iterate through each point and evaluate the gain for splitting at that point\n",
        "        for i in range(self.sample_size - 1):\n",
        "            grad_i, curv_i, val_i, next_val = sorted_gradients[i], sorted_curvatures[i], sorted_feature_values[i], sorted_feature_values[i + 1]\n",
        "            left_gradient += grad_i\n",
        "            right_gradient -= grad_i\n",
        "            left_curvature += curv_i\n",
        "            right_curvature -= curv_i\n",
        "\n",
        "            # Skip splits that don't satisfy minimum child weight or result in identical splits\n",
        "            if left_curvature < self.min_child_weight or val_i == next_val:\n",
        "                continue\n",
        "            if right_curvature < self.min_child_weight:\n",
        "                break\n",
        "\n",
        "            # Calculate the gain from splitting the node at this point\n",
        "            gain = 0.5 * ((left_gradient**2 / (left_curvature + self.regularization))\n",
        "                          + (right_gradient**2 / (right_curvature + self.regularization))\n",
        "                          - (total_gradient**2 / (total_curvature + self.regularization))\n",
        "                         ) - self.split_penalty / 2\n",
        "\n",
        "            # If this split provides a better gain, record it as the best split\n",
        "            if gain > self.best_gain:\n",
        "                self.best_split_feature = feature_idx\n",
        "                self.best_gain = gain\n",
        "                self.split_threshold = (val_i + next_val) / 2\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''Makes predictions for the input data by recursively traversing the decision tree.'''\n",
        "        return np.array([self._predict_single(row) for i, row in X.iterrows()])\n",
        "\n",
        "    def _predict_single(self, row):\n",
        "        '''Predicts the value for a single input row by following the tree structure.'''\n",
        "        if self.is_leaf_node:\n",
        "            return self.prediction_value\n",
        "        if row[self.best_split_feature] <= self.split_threshold:\n",
        "            return self.left_child._predict_single(row)\n",
        "        else:\n",
        "            return self.right_child._predict_single(row)\n"
      ]
    }
  ]
}